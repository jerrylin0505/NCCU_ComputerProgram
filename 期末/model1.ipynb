{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Import some useful packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For data preprocessing\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Layers for FNN\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('euw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.columns[0], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df[df.columns[1:]]\n",
    "y_train = df[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "      top_win_rate  top_lane_kill_rate   top_kda  top_kill_participate  \\\n0           0.4849              0.4237  0.513369              0.513716   \n1           0.5014              0.5535  0.485714              0.482718   \n2           0.5082              0.6148  0.593381              0.524331   \n3           0.5122              0.3297  0.400763              0.482803   \n4           0.4865              0.5265  0.553299              0.533696   \n...            ...                 ...       ...                   ...   \n2860        0.5037              0.4119  0.437975              0.481572   \n2861        0.5089              0.5269  0.570194              0.521429   \n2862        0.6306              0.4598  0.603734              0.539928   \n2863        0.5593              0.6230  0.568928              0.501788   \n2864        0.5228              0.6436  0.622328              0.561549   \n\n      top_damage  jg_win_rate    jg_kda  jg_kill_participate  jg_damage  \\\n0       0.487368       0.4936  0.499002             0.502503   0.469126   \n1       0.515910       0.4980  0.499002             0.507630   0.486446   \n2       0.516326       0.5107  0.522449             0.518191   0.447081   \n3       0.591403       0.5412  0.515658             0.507614   0.481548   \n4       0.503255       0.4967  0.466377             0.510121   0.493189   \n...          ...          ...       ...                  ...        ...   \n2860    0.478134       0.4867  0.428571             0.488417   0.467855   \n2861    0.497815       0.4907  0.446397             0.490079   0.606387   \n2862    0.505202       0.5020  0.500998             0.492370   0.513554   \n2863    0.437633       0.5270  0.489945             0.480569   0.607274   \n2864    0.506723       0.4877  0.468619             0.508016   0.494135   \n\n      mid_win_rate  ...  mid_damage  adc_win_rate  adc_lane_kill_rate  \\\n0           0.4875  ...    0.468713        0.5423              0.4937   \n1           0.5556  ...    0.572448        0.5036              0.5891   \n2           0.5147  ...    0.424833        0.4779              0.5416   \n3           0.5665  ...    0.502442        0.5017              0.4662   \n4           0.5578  ...    0.541907        0.4964              0.4109   \n...            ...  ...         ...           ...                 ...   \n2860        0.4899  ...    0.420969        0.4837              0.4926   \n2861        0.5388  ...    0.533293        0.4854              0.3485   \n2862        0.4835  ...    0.477617        0.4998              0.4270   \n2863        0.5408  ...    0.527127        0.4926              0.4956   \n2864        0.5230  ...    0.474140        0.5295              0.5891   \n\n       adc_kda  adc_kill_participate  adc_damage  sup_win_rate  \\\n0     0.528155              0.516958    0.516174        0.4953   \n1     0.527451              0.525762    0.512016        0.4953   \n2     0.482243              0.494781    0.456389        0.4724   \n3     0.479125              0.490216    0.463594        0.5432   \n4     0.472549              0.474238    0.487984        0.5105   \n...        ...                   ...         ...           ...   \n2860  0.436893              0.486430    0.469174        0.4906   \n2861  0.397490              0.464968    0.460317        0.4951   \n2862  0.534783              0.506098    0.491764        0.5546   \n2863  0.445808              0.479879    0.440301        0.5362   \n2864  0.480331              0.504634    0.459211        0.5160   \n\n      sup_lane_kill_rate   sup_kda  sup_kill_participate  \n0                 0.5846  0.449275              0.478774  \n1                 0.4914  0.478516              0.475356  \n2                 0.4548  0.409814              0.480403  \n3                 0.3825  0.501873              0.530335  \n4                 0.4542  0.494983              0.509524  \n...                  ...       ...                   ...  \n2860              0.4172  0.481216              0.393983  \n2861              0.6011  0.493601              0.462641  \n2862              0.3859  0.496689              0.531477  \n2863              0.4975  0.588608              0.502518  \n2864              0.3429  0.532819              0.447011  \n\n[2865 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>top_win_rate</th>\n      <th>top_lane_kill_rate</th>\n      <th>top_kda</th>\n      <th>top_kill_participate</th>\n      <th>top_damage</th>\n      <th>jg_win_rate</th>\n      <th>jg_kda</th>\n      <th>jg_kill_participate</th>\n      <th>jg_damage</th>\n      <th>mid_win_rate</th>\n      <th>...</th>\n      <th>mid_damage</th>\n      <th>adc_win_rate</th>\n      <th>adc_lane_kill_rate</th>\n      <th>adc_kda</th>\n      <th>adc_kill_participate</th>\n      <th>adc_damage</th>\n      <th>sup_win_rate</th>\n      <th>sup_lane_kill_rate</th>\n      <th>sup_kda</th>\n      <th>sup_kill_participate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.4849</td>\n      <td>0.4237</td>\n      <td>0.513369</td>\n      <td>0.513716</td>\n      <td>0.487368</td>\n      <td>0.4936</td>\n      <td>0.499002</td>\n      <td>0.502503</td>\n      <td>0.469126</td>\n      <td>0.4875</td>\n      <td>...</td>\n      <td>0.468713</td>\n      <td>0.5423</td>\n      <td>0.4937</td>\n      <td>0.528155</td>\n      <td>0.516958</td>\n      <td>0.516174</td>\n      <td>0.4953</td>\n      <td>0.5846</td>\n      <td>0.449275</td>\n      <td>0.478774</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.5014</td>\n      <td>0.5535</td>\n      <td>0.485714</td>\n      <td>0.482718</td>\n      <td>0.515910</td>\n      <td>0.4980</td>\n      <td>0.499002</td>\n      <td>0.507630</td>\n      <td>0.486446</td>\n      <td>0.5556</td>\n      <td>...</td>\n      <td>0.572448</td>\n      <td>0.5036</td>\n      <td>0.5891</td>\n      <td>0.527451</td>\n      <td>0.525762</td>\n      <td>0.512016</td>\n      <td>0.4953</td>\n      <td>0.4914</td>\n      <td>0.478516</td>\n      <td>0.475356</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.5082</td>\n      <td>0.6148</td>\n      <td>0.593381</td>\n      <td>0.524331</td>\n      <td>0.516326</td>\n      <td>0.5107</td>\n      <td>0.522449</td>\n      <td>0.518191</td>\n      <td>0.447081</td>\n      <td>0.5147</td>\n      <td>...</td>\n      <td>0.424833</td>\n      <td>0.4779</td>\n      <td>0.5416</td>\n      <td>0.482243</td>\n      <td>0.494781</td>\n      <td>0.456389</td>\n      <td>0.4724</td>\n      <td>0.4548</td>\n      <td>0.409814</td>\n      <td>0.480403</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.5122</td>\n      <td>0.3297</td>\n      <td>0.400763</td>\n      <td>0.482803</td>\n      <td>0.591403</td>\n      <td>0.5412</td>\n      <td>0.515658</td>\n      <td>0.507614</td>\n      <td>0.481548</td>\n      <td>0.5665</td>\n      <td>...</td>\n      <td>0.502442</td>\n      <td>0.5017</td>\n      <td>0.4662</td>\n      <td>0.479125</td>\n      <td>0.490216</td>\n      <td>0.463594</td>\n      <td>0.5432</td>\n      <td>0.3825</td>\n      <td>0.501873</td>\n      <td>0.530335</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.4865</td>\n      <td>0.5265</td>\n      <td>0.553299</td>\n      <td>0.533696</td>\n      <td>0.503255</td>\n      <td>0.4967</td>\n      <td>0.466377</td>\n      <td>0.510121</td>\n      <td>0.493189</td>\n      <td>0.5578</td>\n      <td>...</td>\n      <td>0.541907</td>\n      <td>0.4964</td>\n      <td>0.4109</td>\n      <td>0.472549</td>\n      <td>0.474238</td>\n      <td>0.487984</td>\n      <td>0.5105</td>\n      <td>0.4542</td>\n      <td>0.494983</td>\n      <td>0.509524</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2860</th>\n      <td>0.5037</td>\n      <td>0.4119</td>\n      <td>0.437975</td>\n      <td>0.481572</td>\n      <td>0.478134</td>\n      <td>0.4867</td>\n      <td>0.428571</td>\n      <td>0.488417</td>\n      <td>0.467855</td>\n      <td>0.4899</td>\n      <td>...</td>\n      <td>0.420969</td>\n      <td>0.4837</td>\n      <td>0.4926</td>\n      <td>0.436893</td>\n      <td>0.486430</td>\n      <td>0.469174</td>\n      <td>0.4906</td>\n      <td>0.4172</td>\n      <td>0.481216</td>\n      <td>0.393983</td>\n    </tr>\n    <tr>\n      <th>2861</th>\n      <td>0.5089</td>\n      <td>0.5269</td>\n      <td>0.570194</td>\n      <td>0.521429</td>\n      <td>0.497815</td>\n      <td>0.4907</td>\n      <td>0.446397</td>\n      <td>0.490079</td>\n      <td>0.606387</td>\n      <td>0.5388</td>\n      <td>...</td>\n      <td>0.533293</td>\n      <td>0.4854</td>\n      <td>0.3485</td>\n      <td>0.397490</td>\n      <td>0.464968</td>\n      <td>0.460317</td>\n      <td>0.4951</td>\n      <td>0.6011</td>\n      <td>0.493601</td>\n      <td>0.462641</td>\n    </tr>\n    <tr>\n      <th>2862</th>\n      <td>0.6306</td>\n      <td>0.4598</td>\n      <td>0.603734</td>\n      <td>0.539928</td>\n      <td>0.505202</td>\n      <td>0.5020</td>\n      <td>0.500998</td>\n      <td>0.492370</td>\n      <td>0.513554</td>\n      <td>0.4835</td>\n      <td>...</td>\n      <td>0.477617</td>\n      <td>0.4998</td>\n      <td>0.4270</td>\n      <td>0.534783</td>\n      <td>0.506098</td>\n      <td>0.491764</td>\n      <td>0.5546</td>\n      <td>0.3859</td>\n      <td>0.496689</td>\n      <td>0.531477</td>\n    </tr>\n    <tr>\n      <th>2863</th>\n      <td>0.5593</td>\n      <td>0.6230</td>\n      <td>0.568928</td>\n      <td>0.501788</td>\n      <td>0.437633</td>\n      <td>0.5270</td>\n      <td>0.489945</td>\n      <td>0.480569</td>\n      <td>0.607274</td>\n      <td>0.5408</td>\n      <td>...</td>\n      <td>0.527127</td>\n      <td>0.4926</td>\n      <td>0.4956</td>\n      <td>0.445808</td>\n      <td>0.479879</td>\n      <td>0.440301</td>\n      <td>0.5362</td>\n      <td>0.4975</td>\n      <td>0.588608</td>\n      <td>0.502518</td>\n    </tr>\n    <tr>\n      <th>2864</th>\n      <td>0.5228</td>\n      <td>0.6436</td>\n      <td>0.622328</td>\n      <td>0.561549</td>\n      <td>0.506723</td>\n      <td>0.4877</td>\n      <td>0.468619</td>\n      <td>0.508016</td>\n      <td>0.494135</td>\n      <td>0.5230</td>\n      <td>...</td>\n      <td>0.474140</td>\n      <td>0.5295</td>\n      <td>0.5891</td>\n      <td>0.480331</td>\n      <td>0.504634</td>\n      <td>0.459211</td>\n      <td>0.5160</td>\n      <td>0.3429</td>\n      <td>0.532819</td>\n      <td>0.447011</td>\n    </tr>\n  </tbody>\n</table>\n<p>2865 rows × 23 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(2865,)"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1., 0.], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "y_train[1003]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "FC_layers = [Dense(units=256, input_dim=23, activation='relu'),\n",
    "             Dense(units=128, activation = 'relu'),\n",
    "             Dense(units=2, activation='softmax')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 256)               6144      \n_________________________________________________________________\ndense_1 (Dense)              (None, 128)               32896     \n_________________________________________________________________\ndense_2 (Dense)              (None, 2)                 258       \n=================================================================\nTotal params: 39,298\nTrainable params: 39,298\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model = Sequential(FC_layers)\n",
    "# model.add(Dense(87, input_dim=784, activation='relu'))\n",
    "# model.add(Dense(128,activation = 'relu'))\n",
    "# model.add(Dense(10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer=SGD(lr=0.1), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 2387 samples, validate on 478 samples\nEpoch 1/50\n2387/2387 [==============================] - 0s 65us/sample - loss: 0.2495 - accuracy: 0.5245 - val_loss: 0.2513 - val_accuracy: 0.5021\nEpoch 2/50\n2387/2387 [==============================] - 0s 74us/sample - loss: 0.2497 - accuracy: 0.5178 - val_loss: 0.2506 - val_accuracy: 0.5000\nEpoch 3/50\n2387/2387 [==============================] - 0s 67us/sample - loss: 0.2491 - accuracy: 0.5266 - val_loss: 0.2507 - val_accuracy: 0.4623\nEpoch 4/50\n2387/2387 [==============================] - 0s 67us/sample - loss: 0.2494 - accuracy: 0.5224 - val_loss: 0.2516 - val_accuracy: 0.5021\nEpoch 5/50\n2387/2387 [==============================] - 0s 61us/sample - loss: 0.2496 - accuracy: 0.5249 - val_loss: 0.2507 - val_accuracy: 0.4937\nEpoch 6/50\n2387/2387 [==============================] - 0s 65us/sample - loss: 0.2491 - accuracy: 0.5207 - val_loss: 0.2516 - val_accuracy: 0.5021\nEpoch 7/50\n2387/2387 [==============================] - 0s 60us/sample - loss: 0.2496 - accuracy: 0.5178 - val_loss: 0.2509 - val_accuracy: 0.4895\nEpoch 8/50\n2387/2387 [==============================] - 0s 65us/sample - loss: 0.2488 - accuracy: 0.5279 - val_loss: 0.2511 - val_accuracy: 0.5042\nEpoch 9/50\n2387/2387 [==============================] - 0s 62us/sample - loss: 0.2483 - accuracy: 0.5354 - val_loss: 0.2519 - val_accuracy: 0.5021\nEpoch 10/50\n2387/2387 [==============================] - 0s 63us/sample - loss: 0.2491 - accuracy: 0.5266 - val_loss: 0.2508 - val_accuracy: 0.5021\nEpoch 11/50\n2387/2387 [==============================] - 0s 63us/sample - loss: 0.2488 - accuracy: 0.5233 - val_loss: 0.2508 - val_accuracy: 0.5063\nEpoch 12/50\n2387/2387 [==============================] - 0s 75us/sample - loss: 0.2489 - accuracy: 0.5295 - val_loss: 0.2508 - val_accuracy: 0.5063\nEpoch 13/50\n2387/2387 [==============================] - 0s 74us/sample - loss: 0.2493 - accuracy: 0.5283 - val_loss: 0.2508 - val_accuracy: 0.5084\nEpoch 14/50\n2387/2387 [==============================] - 0s 65us/sample - loss: 0.2488 - accuracy: 0.5237 - val_loss: 0.2508 - val_accuracy: 0.4895\nEpoch 15/50\n2387/2387 [==============================] - 0s 66us/sample - loss: 0.2488 - accuracy: 0.5212 - val_loss: 0.2511 - val_accuracy: 0.4895\nEpoch 16/50\n2387/2387 [==============================] - 0s 60us/sample - loss: 0.2490 - accuracy: 0.5178 - val_loss: 0.2509 - val_accuracy: 0.4791\nEpoch 17/50\n2387/2387 [==============================] - 0s 68us/sample - loss: 0.2486 - accuracy: 0.5304 - val_loss: 0.2510 - val_accuracy: 0.5000\nEpoch 18/50\n2387/2387 [==============================] - 0s 61us/sample - loss: 0.2487 - accuracy: 0.5249 - val_loss: 0.2520 - val_accuracy: 0.4979\nEpoch 19/50\n2387/2387 [==============================] - 0s 64us/sample - loss: 0.2487 - accuracy: 0.5295 - val_loss: 0.2515 - val_accuracy: 0.4833\nEpoch 20/50\n2387/2387 [==============================] - 0s 74us/sample - loss: 0.2489 - accuracy: 0.5300 - val_loss: 0.2524 - val_accuracy: 0.4979\nEpoch 21/50\n2387/2387 [==============================] - 0s 70us/sample - loss: 0.2489 - accuracy: 0.5228 - val_loss: 0.2510 - val_accuracy: 0.5063\nEpoch 22/50\n2387/2387 [==============================] - 0s 62us/sample - loss: 0.2492 - accuracy: 0.5237 - val_loss: 0.2509 - val_accuracy: 0.5042\nEpoch 23/50\n2387/2387 [==============================] - 0s 64us/sample - loss: 0.2495 - accuracy: 0.5153 - val_loss: 0.2511 - val_accuracy: 0.4770\nEpoch 24/50\n2387/2387 [==============================] - 0s 59us/sample - loss: 0.2492 - accuracy: 0.5207 - val_loss: 0.2510 - val_accuracy: 0.4895\nEpoch 25/50\n2387/2387 [==============================] - 0s 74us/sample - loss: 0.2485 - accuracy: 0.5295 - val_loss: 0.2516 - val_accuracy: 0.5000\nEpoch 26/50\n2387/2387 [==============================] - 0s 66us/sample - loss: 0.2490 - accuracy: 0.5241 - val_loss: 0.2510 - val_accuracy: 0.4916\nEpoch 27/50\n2387/2387 [==============================] - 0s 64us/sample - loss: 0.2490 - accuracy: 0.5245 - val_loss: 0.2537 - val_accuracy: 0.5042\nEpoch 28/50\n2387/2387 [==============================] - 0s 68us/sample - loss: 0.2487 - accuracy: 0.5128 - val_loss: 0.2515 - val_accuracy: 0.4895\nEpoch 29/50\n2387/2387 [==============================] - 0s 66us/sample - loss: 0.2489 - accuracy: 0.5241 - val_loss: 0.2511 - val_accuracy: 0.4854\nEpoch 30/50\n2387/2387 [==============================] - 0s 65us/sample - loss: 0.2485 - accuracy: 0.5300 - val_loss: 0.2510 - val_accuracy: 0.5084\nEpoch 31/50\n2387/2387 [==============================] - 0s 66us/sample - loss: 0.2485 - accuracy: 0.5199 - val_loss: 0.2521 - val_accuracy: 0.4979\nEpoch 32/50\n2387/2387 [==============================] - 0s 69us/sample - loss: 0.2482 - accuracy: 0.5354 - val_loss: 0.2512 - val_accuracy: 0.5021\nEpoch 33/50\n2387/2387 [==============================] - 0s 65us/sample - loss: 0.2485 - accuracy: 0.5249 - val_loss: 0.2513 - val_accuracy: 0.5000\nEpoch 34/50\n2387/2387 [==============================] - 0s 63us/sample - loss: 0.2477 - accuracy: 0.5496 - val_loss: 0.2533 - val_accuracy: 0.4979\nEpoch 35/50\n2387/2387 [==============================] - 0s 64us/sample - loss: 0.2481 - accuracy: 0.5270 - val_loss: 0.2541 - val_accuracy: 0.4958\nEpoch 36/50\n2387/2387 [==============================] - 0s 62us/sample - loss: 0.2492 - accuracy: 0.5266 - val_loss: 0.2569 - val_accuracy: 0.5042\nEpoch 37/50\n2387/2387 [==============================] - 0s 70us/sample - loss: 0.2485 - accuracy: 0.5249 - val_loss: 0.2512 - val_accuracy: 0.4979\nEpoch 38/50\n2387/2387 [==============================] - 0s 61us/sample - loss: 0.2483 - accuracy: 0.5358 - val_loss: 0.2516 - val_accuracy: 0.4854\nEpoch 39/50\n2387/2387 [==============================] - 0s 65us/sample - loss: 0.2492 - accuracy: 0.5253 - val_loss: 0.2513 - val_accuracy: 0.4833\nEpoch 40/50\n2387/2387 [==============================] - 0s 66us/sample - loss: 0.2484 - accuracy: 0.5295 - val_loss: 0.2515 - val_accuracy: 0.4895\nEpoch 41/50\n2387/2387 [==============================] - 0s 73us/sample - loss: 0.2483 - accuracy: 0.5304 - val_loss: 0.2513 - val_accuracy: 0.5000\nEpoch 42/50\n2387/2387 [==============================] - 0s 63us/sample - loss: 0.2479 - accuracy: 0.5400 - val_loss: 0.2522 - val_accuracy: 0.4937\nEpoch 43/50\n2387/2387 [==============================] - 0s 63us/sample - loss: 0.2482 - accuracy: 0.5475 - val_loss: 0.2526 - val_accuracy: 0.5063\nEpoch 44/50\n2387/2387 [==============================] - 0s 68us/sample - loss: 0.2482 - accuracy: 0.5245 - val_loss: 0.2518 - val_accuracy: 0.4791\nEpoch 45/50\n2387/2387 [==============================] - 0s 61us/sample - loss: 0.2483 - accuracy: 0.5178 - val_loss: 0.2520 - val_accuracy: 0.4895\nEpoch 46/50\n2387/2387 [==============================] - 0s 67us/sample - loss: 0.2488 - accuracy: 0.5287 - val_loss: 0.2514 - val_accuracy: 0.4854\nEpoch 47/50\n2387/2387 [==============================] - 0s 64us/sample - loss: 0.2484 - accuracy: 0.5245 - val_loss: 0.2529 - val_accuracy: 0.4916\nEpoch 48/50\n2387/2387 [==============================] - 0s 67us/sample - loss: 0.2483 - accuracy: 0.5300 - val_loss: 0.2596 - val_accuracy: 0.5042\nEpoch 49/50\n2387/2387 [==============================] - 0s 64us/sample - loss: 0.2488 - accuracy: 0.5308 - val_loss: 0.2515 - val_accuracy: 0.4833\nEpoch 50/50\n2387/2387 [==============================] - 0s 63us/sample - loss: 0.2488 - accuracy: 0.5203 - val_loss: 0.2515 - val_accuracy: 0.5084\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1c4709d6c88>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=50, epochs=50, validation_split = 1/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}